)
)
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
shiny::runApp()
install.package("corrplot")
install.package("corrplot")
install.packages("corrplot")
runApp()
runApp()
ui_model_predictors_influence <- sidebarLayout(
sidebarPanel(
uiOutput('predictors_influence'),
actionButton(
inputId = "submit_loc",
label = "Submit"
)
),
mainPanel(
h2("Predictors influence"),
fluidRow(
plotOutput("influence"),
)
)
)
runApp()
model_result_summary
ui_model_summary <- tabItem(
tabsetPanel(
tabPanel("Summary", ui_model_result_summary),
tabPanel("Influence of predictors", ui_model_predictors_influence)
))
runApp()
runApp()
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
cache=TRUE, size="scriptsize",fig.width=6, fig.height=5)
library(reticulate)
runApp('~/Desktop/RandomForestVisualization/app')
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
cache=TRUE, size="scriptsize",fig.width=6, fig.height=5)
library(reticulate)
#use_python("/Library/Frameworks/Python.framework/Versions/3.6/bin/python3", required = T)
knitr::knit_engines$set(python.reticulate =  TRUE)
#py_install("matplotlib")
#py_install("scikit-learn")
install.packages("ggfortify")
library(magrittr)
library(knitr)
library(rmarkdown)
library(ggplot2)
library(ggfortify)
library(MASS)
library(dplyr)
library(randomForest)
library(tidyverse)
install.packages("ggfortify")
library(magrittr)
library(knitr)
library(rmarkdown)
library(ggplot2)
library(ggfortify)
library(MASS)
library(dplyr)
library(randomForest)
library(caret)
library(cluster)
library(factoextra)
id <- "1GNbIhjdhuwPOBr0Qz82JMkdjUVBuSoZd"
tennis <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download",id), header = T)
# test and train set
n = dim(tennis)[1]
n2 = n*(3/4)
set.seed(1234)
train = sample(c(1:n), replace = F)[1:n2]
# reduction to two variables
tennis$ACEdiff = tennis$ACE.1 - tennis$ACE.2
tennis$UFEdiff = tennis$UFE.1 - tennis$UFE.2
head(tennis)
names(tennis)
tennisTest = tennis[-train, ]
tennisTrain = tennis[train, ]
r.tennis2 = glm(Result ~ ACEdiff + UFEdiff, data = tennisTrain)
summary(r.tennis2)
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
class = c("0","1")
op <- par(mfrow=c(length(vars), length(class)))
for (i in 1:length(vars)) {
print(i)
for (j in 1:length(class)) {
op <- partialPlot(model, tennisTrain, vars[i], class[j],
main=paste("Partial Dependence on", vars[i]))
}
}
print(op)
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
class = c("0","1")
op <- par(mfrow=c(length(vars), length(class)))
for (i in 1:length(vars)) {
print(i)
for (j in 1:length(class)) {
op <- partialPlot(model, tennisTrain, vars[i], class[j],
main=paste("Partial Dependence on", vars[i]))
}
}
print(op)
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
class = c("0","1")
op <- par(mfrow=c(length(vars), length(class)))
for (i in 1:length(vars)) {
print(i)
for (j in 1:length(class)) {
op <- partialPlot(model, tennisTrain, vars[i], class[j],
main=paste("Partial Dependence on", vars[i]))
}
}
par(op)
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
classes = c(unique(tennisTrain[,"Result"]))
print(classes)
op <- par(mfrow=c(length(vars), length(classes)))
for (var in vars) {
for (class in classes) {
op <- partialPlot(model, tennisTrain, var, class,
main=paste("Partial Dependence on", var))
}
}
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
classes = c(unique(tennisTrain[,"Result"]))
print(classes)
op <- par(mfrow=c(length(vars), length(classes)))
for (var in vars) {
for (class in classes) {
op <- partialPlot(model, tennisTrain, c(var), class,
main=paste("Partial Dependence on", var))
}
}
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
classes = c(unique(tennisTrain[,"Result"]))
print(classes)
op <- par(mfrow=c(length(vars), length(classes)))
for (var in vars) {
for (class in classes) {
print(var)
op <- partialPlot(model, tennisTrain, var, class, main=paste("Partial Dependence on", var))
}
}
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
classes = c(unique(tennisTrain[,"Result"]))
print(classes)
op <- par(mfrow=c(length(vars), length(classes)))
for (var in vars) {
for (class in classes) {
print(var)
op <- partialPlot(model, tennisTrain, var, c(class), main=paste("Partial Dependence on", var))
}
}
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
classes = c(unique(tennisTrain[,"Result"]))
print(classes)
op <- par(mfrow=c(length(vars), length(classes)))
for (var in vars) {
for (class in classes) {
print(var)
op <- partialPlot(model, tennisTrain, var, quote(class), main=paste("Partial Dependence on", var))
}
}
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
classes = c(unique(tennisTrain[,"Result"]))
print(classes)
op <- par(mfrow=c(length(vars), length(classes)))
for (var in vars) {
for (class in classes) {
print(var)
op <- partialPlot(model, tennisTrain, var, as.factor(class), main=paste("Partial Dependence on", var))
}
}
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
classes = c(unique(tennisTrain[,"Result"]))
print(classes)
op <- par(mfrow=c(length(vars), length(classes)))
for (var in vars) {
for (class in classes) {
print(var)
op <- partialPlot(model, tennisTrain, var, "1", main=paste("Partial Dependence on", var))
}
}
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
classes = c(unique(tennisTrain[,"Result"]))
print(classes)
op <- par(mfrow=c(length(vars), length(classes)))
for (var in vars) {
for (class in classes) {
print(var)
op <- partialPlot(model, tennisTrain, "ACEdiff", "1", main=paste("Partial Dependence on", var))
}
}
par(op)
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
classes = c(unique(tennisTrain[,"Result"]))
print(classes)
op <- par(mfrow=c(length(vars), length(classes)))
for (var in vars) {
for (class in classes) {
print(var)
op <- partialPlot(model, tennisTrain, "ACEdiff", class, main=paste("Partial Dependence on", var))
}
}
par(op)
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
classes = c(unique(tennisTrain[,"Result"]))
print(classes)
op <- par(mfrow=c(length(vars), length(classes)))
for (var in vars) {
for (class in classes) {
print(var)
op <- partialPlot(model, tennisTrain, c("ACEdiff"), class, main=paste("Partial Dependence on", var))
}
}
par(op)
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
classes = c(unique(tennisTrain[,"Result"]))
print(classes)
op <- par(mfrow=c(length(vars), length(classes)))
for (var in vars) {
for (class in classes) {
print(var)
op <- partialPlot(model, tennisTrain, c(var), class, main=paste("Partial Dependence on", var))
}
}
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
classes = c(unique(tennisTrain[,"Result"]))
print(classes)
op <- par(mfrow=c(length(vars), length(classes)))
for (var in vars) {
for (class in classes) {
print(var)
op <- partialPlot(model, tennisTrain, c(var)[0], class, main=paste("Partial Dependence on", var))
}
}
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
classes = c(unique(tennisTrain[,"Result"]))
print(classes)
op <- par(mfrow=c(length(vars), length(classes)))
for (i in 1:length(vars)) {
for (class in classes) {
print(vars[i])
op <- partialPlot(model, tennisTrain, vars[i], class, main=paste("Partial Dependence on", vars[i]))
}
}
par(op)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = colnames(data[,c("ACEdiff","UFEdiff")])
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = colnames(tennisTrain[,c("ACEdiff","UFEdiff")])
classes = c(unique(tennisTrain[,"Result"]))
print(classes)
op <- par(mfrow=c(length(vars), length(classes)))
for (i in 1:length(vars)) {
for (class in classes) {
print(vars[i])
op <- partialPlot(model, tennisTrain, vars[i], class, main=paste("Partial Dependence on", vars[i]))
}
}
par(op)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
ui_model_tree <-
mainPanel(
fluidRow(
h2("Tree with rpart library"),
plotOutput("tree")
)
)
runApp()
runApp()
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
cache=TRUE, size="scriptsize",fig.width=6, fig.height=5)
library(reticulate)
#use_python("/Library/Frameworks/Python.framework/Versions/3.6/bin/python3", required = T)
knitr::knit_engines$set(python.reticulate =  TRUE)
#py_install("matplotlib")
#py_install("scikit-learn")
#The best model is
model <-  randomForest(as.factor(tennisTrain[,"Result"]) ~ ., data=tennisTrain[,c("ACEdiff","UFEdiff")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
id <- "1GNbIhjdhuwPOBr0Qz82JMkdjUVBuSoZd"
tennis <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download",id), header = T)
# test and train set
n = dim(tennis)[1]
n2 = n*(3/4)
set.seed(1234)
train = sample(c(1:n), replace = F)[1:n2]
# reduction to two variables
tennis$ACEdiff = tennis$ACE.1 - tennis$ACE.2
tennis$UFEdiff = tennis$UFE.1 - tennis$UFE.2
head(tennis)
names(tennis)
tennisTest = tennis[-train, ]
tennisTrain = tennis[train, ]
r.tennis2 = glm(Result ~ ACEdiff + UFEdiff, data = tennisTrain)
summary(r.tennis2)
#The best model is
model <-  randomForest(as.factor(tennisTrain[,"Result"]) ~ ., data=tennisTrain[,c("ACEdiff","UFEdiff")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
rpart.plot(model, tweak = 1.5)
#The best model is
model <-  randomForest(as.factor(tennisTrain[,"Result"]) ~ ., data=tennisTrain[,c("ACEdiff","UFEdiff")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
plot(model, tweak = 1.5)
shiny::runApp()
ui_model_predictors <- sidebarLayout(
sidebarPanel(
),
mainPanel(
)
)
runApp()
ui_data_summary <- sidebarLayout(
sidebarPanel(
selectInput("distrib_type", "Plot type", c("Boxplot" = "boxplot", "Histogram" = "histogram")),
conditionalPanel("input.distrib_type == 'boxplot'",
selectInput("distrib_box_form", "Form", c("box", "violin")),
helpText("Choose the variable to plot. Drag and drop to reorder."),
selectizeInput("distrib_box_vars", "", c("Loading..."), multiple = T,
options = list(plugins = list("remove_button", "drag_drop")))
),
conditionalPanel("input.distrib_type == 'histogram'",
helpText("Choose the variable to plot. Drag and drop to reorder."),
selectizeInput("distrib_hist_vars", "", c("Loading..."), multiple = T,
options = list(plugins = list("remove_button", "drag_drop"))),
selectInput("distrib_hist_freq", "Frequency/Density", c("Frequency", "Density")),
selectInput("distrib_hist_space", "Continuous/Discrete", c("Continous", "Discrete"))
)
),
mainPanel(
tabsetPanel(
tabPanel("Distribution plots",
plotOutput("distrib_out")),
tabPanel("Distribution table",
DT::dataTableOutput("predictors_summary"))
)
)
)
runApp()
library(shiny)
library(shinydashboard)
library(fresh)
library(DT)
library(randomForest)
library(ggplot2)
library(ggfortify)
library(MASS)
library(dplyr)
library(corrplot)
library(reshape2)
library(rpart)
library(rpart.plot)
library(party)
library(ggraph)
library(igraph)
library(pdp)
library(caret)
library(e1071)
library(data.table)
id <- "1GNbIhjdhuwPOBr0Qz82JMkdjUVBuSoZd"
tennis <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download",id), header = T)
# test and train set
n = dim(tennis)[1]
n2 = n*(3/4)
set.seed(1234)
train = sample(c(1:n), replace = F)[1:n2]
# reduction to two variables
tennis$ACEdiff = tennis$ACE.1 - tennis$ACE.2
tennis$UFEdiff = tennis$UFE.1 - tennis$UFE.2
head(tennis)
names(tennis)
tennisTest = tennis[-train, ]
tennisTrain = tennis[train, ]
tm <- rpart(as.factor(tennisTrain[,"Result"]) ~ ., data=tennisTrain[,c("ACEdiff","UFEdiff")], method = "class")
do.call("partial", list(tm, pred.var = c("ACEdiff","UFEdiff"),
data = x_train, plot = TRUE, rug = TRUE,
chull = TRUE, plot.engine = "ggplot2")) +
labs(title = paste("Partial Dependence of", c("ACEdiff","UFEdiff")))
do.call("partial", list(tm, pred.var = c("ACEdiff","UFEdiff"),
data = tennisTrain, plot = TRUE, rug = TRUE,
chull = TRUE, plot.engine = "ggplot2")) +
labs(title = paste("Partial Dependence of", c("ACEdiff","UFEdiff")))
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
