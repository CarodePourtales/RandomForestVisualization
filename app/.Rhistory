model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
class = c("0","1")
op <- par(mfrow=c(length(vars), length(class)))
for (i in 1:length(vars)) {
print(i)
for (j in 1:length(class)) {
op[i,j] <- partialPlot(model, tennisTrain, vars[i], class[j],
main=paste("Partial Dependence on", vars[i]))
}
}
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
class = c("0","1")
op <- par(mfrow=c(length(vars), length(class)))
for (i in 1:length(vars)) {
print(i)
for (j in 1:length(class)) {
op<- partialPlot(model, tennisTrain, vars[i], class[j],
main=paste("Partial Dependence on", vars[i]))
}
}
par(op)
runApp()
runApp()
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
class = c("0","1")
op <- par(mfrow=c(length(vars), length(class)))
for (i in 1:length(vars)) {
print(i)
for (j in 1:length(class)) {
op <- partialPlot(model, tennisTrain, vars[i], class[j],
main=paste("Partial Dependence on", vars[i]))
}
}
print(op)
runApp()
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
class = c("0","1")
op <- par(mfrow=c(length(vars), length(class)))
for (i in 1:length(vars)) {
print(i)
for (j in 1:length(class)) {
op <- partialPlot(model, tennisTrain, vars[i], class[j],
main=paste("Partial Dependence on", vars[i]))
}
}
print(op)
runApp()
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
class = c("0","1")
op <- par(mfrow=c(length(vars), length(class)))
for (i in 1:length(vars)) {
print(i)
for (j in 1:length(class)) {
op <- plot(model, tennisTrain, vars[i], class[j],
main=paste("Partial Dependence on", vars[i]))
}
}
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
class = c("0","1")
op <- par(mfrow=c(length(vars), length(class)))
for (i in 1:length(vars)) {
print(i)
for (j in 1:length(class)) {
op <- partialPlot(model, tennisTrain, vars[i], class[j],
main=paste("Partial Dependence on", vars[i]))
}
}
par(op)
ui_model_result_summary <- sidebarLayout(
sidebarPanel(
uiOutput('class'),
uiOutput('predictors'),
sliderInput('ntree','Number of trees to grow',min=10,max=1000,value=200,step=10),
sliderInput('mtry','Number of variables randomly sampled as candidates at each split',min=1,max=20,value=2,step=1),
sliderInput('nodesize','Minimum size of terminal nodes',min=1,max=50,value=10,step=1),
actionButton(
inputId = "submit_loc",
label = "Submit"
)
),
mainPanel(
h2("Model summary"),
verbatimTextOutput("randomForest"),
fluidRow(
splitLayout(cellWidths = c("100%", "0%"), plotOutput("influence"))
)
)
)
runApp()
runApp()
ui_model_result_summary <- sidebarLayout(
sidebarPanel(
uiOutput('class'),
uiOutput('predictors'),
sliderInput('ntree','Number of trees to grow',min=10,max=1000,value=200,step=10),
sliderInput('mtry','Number of variables randomly sampled as candidates at each split',min=1,max=20,value=2,step=1),
sliderInput('nodesize','Minimum size of terminal nodes',min=1,max=50,value=10,step=1),
actionButton(
inputId = "submit_loc",
label = "Submit"
)
),
mainPanel(
fluidRow(h2("Model summary")),
fluidRow(verbatimTextOutput("randomForest")),
fluidRow(
column(6, plotOutput("influence"))
)
)
)
runApp()
ui_model_result_summary <- sidebarLayout(
sidebarPanel(
uiOutput('class'),
uiOutput('predictors'),
sliderInput('ntree','Number of trees to grow',min=10,max=1000,value=200,step=10),
sliderInput('mtry','Number of variables randomly sampled as candidates at each split',min=1,max=20,value=2,step=1),
sliderInput('nodesize','Minimum size of terminal nodes',min=1,max=50,value=10,step=1),
actionButton(
inputId = "submit_loc",
label = "Submit"
)
),
mainPanel(
fluidRow(h2("Model summary")),
fluidRow(verbatimTextOutput("randomForest")),
fluidRow(column(10, plotOutput("influence"))
)
)
)
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
shiny::runApp()
install.package("corrplot")
install.package("corrplot")
install.packages("corrplot")
runApp()
runApp()
ui_model_predictors_influence <- sidebarLayout(
sidebarPanel(
uiOutput('predictors_influence'),
actionButton(
inputId = "submit_loc",
label = "Submit"
)
),
mainPanel(
h2("Predictors influence"),
fluidRow(
plotOutput("influence"),
)
)
)
runApp()
model_result_summary
ui_model_summary <- tabItem(
tabsetPanel(
tabPanel("Summary", ui_model_result_summary),
tabPanel("Influence of predictors", ui_model_predictors_influence)
))
runApp()
runApp()
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
cache=TRUE, size="scriptsize",fig.width=6, fig.height=5)
library(reticulate)
runApp('~/Desktop/RandomForestVisualization/app')
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
cache=TRUE, size="scriptsize",fig.width=6, fig.height=5)
library(reticulate)
#use_python("/Library/Frameworks/Python.framework/Versions/3.6/bin/python3", required = T)
knitr::knit_engines$set(python.reticulate =  TRUE)
#py_install("matplotlib")
#py_install("scikit-learn")
install.packages("ggfortify")
library(magrittr)
library(knitr)
library(rmarkdown)
library(ggplot2)
library(ggfortify)
library(MASS)
library(dplyr)
library(randomForest)
library(tidyverse)
install.packages("ggfortify")
library(magrittr)
library(knitr)
library(rmarkdown)
library(ggplot2)
library(ggfortify)
library(MASS)
library(dplyr)
library(randomForest)
library(caret)
library(cluster)
library(factoextra)
id <- "1GNbIhjdhuwPOBr0Qz82JMkdjUVBuSoZd"
tennis <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download",id), header = T)
# test and train set
n = dim(tennis)[1]
n2 = n*(3/4)
set.seed(1234)
train = sample(c(1:n), replace = F)[1:n2]
# reduction to two variables
tennis$ACEdiff = tennis$ACE.1 - tennis$ACE.2
tennis$UFEdiff = tennis$UFE.1 - tennis$UFE.2
head(tennis)
names(tennis)
tennisTest = tennis[-train, ]
tennisTrain = tennis[train, ]
r.tennis2 = glm(Result ~ ACEdiff + UFEdiff, data = tennisTrain)
summary(r.tennis2)
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
class = c("0","1")
op <- par(mfrow=c(length(vars), length(class)))
for (i in 1:length(vars)) {
print(i)
for (j in 1:length(class)) {
op <- partialPlot(model, tennisTrain, vars[i], class[j],
main=paste("Partial Dependence on", vars[i]))
}
}
print(op)
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
class = c("0","1")
op <- par(mfrow=c(length(vars), length(class)))
for (i in 1:length(vars)) {
print(i)
for (j in 1:length(class)) {
op <- partialPlot(model, tennisTrain, vars[i], class[j],
main=paste("Partial Dependence on", vars[i]))
}
}
print(op)
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
class = c("0","1")
op <- par(mfrow=c(length(vars), length(class)))
for (i in 1:length(vars)) {
print(i)
for (j in 1:length(class)) {
op <- partialPlot(model, tennisTrain, vars[i], class[j],
main=paste("Partial Dependence on", vars[i]))
}
}
par(op)
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
classes = c(unique(tennisTrain[,"Result"]))
print(classes)
op <- par(mfrow=c(length(vars), length(classes)))
for (var in vars) {
for (class in classes) {
op <- partialPlot(model, tennisTrain, var, class,
main=paste("Partial Dependence on", var))
}
}
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
classes = c(unique(tennisTrain[,"Result"]))
print(classes)
op <- par(mfrow=c(length(vars), length(classes)))
for (var in vars) {
for (class in classes) {
op <- partialPlot(model, tennisTrain, c(var), class,
main=paste("Partial Dependence on", var))
}
}
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
classes = c(unique(tennisTrain[,"Result"]))
print(classes)
op <- par(mfrow=c(length(vars), length(classes)))
for (var in vars) {
for (class in classes) {
print(var)
op <- partialPlot(model, tennisTrain, var, class, main=paste("Partial Dependence on", var))
}
}
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
classes = c(unique(tennisTrain[,"Result"]))
print(classes)
op <- par(mfrow=c(length(vars), length(classes)))
for (var in vars) {
for (class in classes) {
print(var)
op <- partialPlot(model, tennisTrain, var, c(class), main=paste("Partial Dependence on", var))
}
}
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
classes = c(unique(tennisTrain[,"Result"]))
print(classes)
op <- par(mfrow=c(length(vars), length(classes)))
for (var in vars) {
for (class in classes) {
print(var)
op <- partialPlot(model, tennisTrain, var, quote(class), main=paste("Partial Dependence on", var))
}
}
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
classes = c(unique(tennisTrain[,"Result"]))
print(classes)
op <- par(mfrow=c(length(vars), length(classes)))
for (var in vars) {
for (class in classes) {
print(var)
op <- partialPlot(model, tennisTrain, var, as.factor(class), main=paste("Partial Dependence on", var))
}
}
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
classes = c(unique(tennisTrain[,"Result"]))
print(classes)
op <- par(mfrow=c(length(vars), length(classes)))
for (var in vars) {
for (class in classes) {
print(var)
op <- partialPlot(model, tennisTrain, var, "1", main=paste("Partial Dependence on", var))
}
}
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
classes = c(unique(tennisTrain[,"Result"]))
print(classes)
op <- par(mfrow=c(length(vars), length(classes)))
for (var in vars) {
for (class in classes) {
print(var)
op <- partialPlot(model, tennisTrain, "ACEdiff", "1", main=paste("Partial Dependence on", var))
}
}
par(op)
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
classes = c(unique(tennisTrain[,"Result"]))
print(classes)
op <- par(mfrow=c(length(vars), length(classes)))
for (var in vars) {
for (class in classes) {
print(var)
op <- partialPlot(model, tennisTrain, "ACEdiff", class, main=paste("Partial Dependence on", var))
}
}
par(op)
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
classes = c(unique(tennisTrain[,"Result"]))
print(classes)
op <- par(mfrow=c(length(vars), length(classes)))
for (var in vars) {
for (class in classes) {
print(var)
op <- partialPlot(model, tennisTrain, c("ACEdiff"), class, main=paste("Partial Dependence on", var))
}
}
par(op)
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
classes = c(unique(tennisTrain[,"Result"]))
print(classes)
op <- par(mfrow=c(length(vars), length(classes)))
for (var in vars) {
for (class in classes) {
print(var)
op <- partialPlot(model, tennisTrain, c(var), class, main=paste("Partial Dependence on", var))
}
}
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
classes = c(unique(tennisTrain[,"Result"]))
print(classes)
op <- par(mfrow=c(length(vars), length(classes)))
for (var in vars) {
for (class in classes) {
print(var)
op <- partialPlot(model, tennisTrain, c(var)[0], class, main=paste("Partial Dependence on", var))
}
}
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = c("ACEdiff","UFEdiff")
classes = c(unique(tennisTrain[,"Result"]))
print(classes)
op <- par(mfrow=c(length(vars), length(classes)))
for (i in 1:length(vars)) {
for (class in classes) {
print(vars[i])
op <- partialPlot(model, tennisTrain, vars[i], class, main=paste("Partial Dependence on", vars[i]))
}
}
par(op)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = colnames(data[,c("ACEdiff","UFEdiff")])
#The best model is
model <-  randomForest(as.factor(Result) ~ ACEdiff + UFEdiff, data=tennisTrain[,c("ACEdiff","UFEdiff","Result")],
mtry = 2, ntree = 200, nodesize=10, importance=TRUE, proximity=TRUE)
vars = colnames(tennisTrain[,c("ACEdiff","UFEdiff")])
classes = c(unique(tennisTrain[,"Result"]))
print(classes)
op <- par(mfrow=c(length(vars), length(classes)))
for (i in 1:length(vars)) {
for (class in classes) {
print(vars[i])
op <- partialPlot(model, tennisTrain, vars[i], class, main=paste("Partial Dependence on", vars[i]))
}
}
par(op)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
